
'use server';
/**
 * @fileOverview Flow pour l'assistant de chat.
 * - chat: Gère une conversation avec l'IA en utilisant le contexte de l'application.
 */

import { ai } from '@/ai/genkit';
import { z } from 'zod';
import { getIngredientsTool, getPreparationsTool, getRecipesTool } from '../tools/assistant-tools';


const ChatInputSchema = z.object({
    history: z.array(z.object({
        role: z.enum(['user', 'assistant']),
        content: z.string(),
    })),
});
type ChatInput = z.infer<typeof ChatInputSchema>;


const ChatOutputSchema = z.object({
    content: z.string(),
});
type ChatOutput = z.infer<typeof ChatOutputSchema>;


const assistantPrompt = `
Tu es "Le Singulier AI", un assistant expert en gestion de restaurant et en analyse culinaire, créé pour aider le gérant du restaurant "Le Singulier". Ton ton est professionnel et collaboratif.
Ta mission est de répondre aux questions de manière précise en te basant sur les données du restaurant.
Pour toute question concernant les plats, les ingrédients, les coûts, les stocks, les préparations, tu dois IMPÉRATIVEMENT utiliser les outils à ta disposition (\`getRecipesTool\`, \`getIngredientsTool\`, \`getPreparationsTool\`).
Si on te demande la quantité produite d'une préparation, utilise getPreparationsTool qui contient cette information.
Si une donnée semble anormale (ex: coût à 0), signale-le poliment.
Formate tes réponses en Markdown pour une meilleure lisibilité.
`;

const chatFlow = ai.defineFlow(
  {
    name: 'assistantChatFlow',
    inputSchema: ChatInputSchema,
    outputSchema: ChatOutputSchema,
  },
  async (input) => {
    console.log('======== [ASSISTANT FLOW START] ========');
    console.log('Received input:', JSON.stringify(input, null, 2));

    const history = input.history.map(msg => ({
      role: msg.role === 'assistant' ? 'model' as const : 'user' as const,
      content: [{ text: msg.content }],
    }));

    const lastUserMessage = history.pop();
    if (!lastUserMessage || lastUserMessage.role !== 'user') {
        console.error('No valid last user message found.');
        return { content: "Désolé, je n'ai pas reçu de question valide." };
    }
    
    console.log('Last user message:', lastUserMessage.content[0].text);
    console.log('History being sent to model:', JSON.stringify(history, null, 2));

    const { output } = await ai.generate({
      model: 'googleai/gemini-2.5-flash',
      prompt: lastUserMessage.content[0].text,
      system: assistantPrompt,
      tools: [getRecipesTool, getPreparationsTool, getIngredientsTool],
      history: history,
    });
    
    console.log('\n======== [RAW MODEL OUTPUT] ========');
    console.log(JSON.stringify(output, null, 2));
    console.log('====================================\n');

    let textResponse = '';
    if (output?.content?.parts) {
        for (const part of output.content.parts) {
            if (part.text) {
                console.log('Found text part:', part.text);
                textResponse += part.text;
            } else if (part.toolRequest) {
                 console.log('Found toolRequest part:', part.toolRequest);
                 // Genkit should handle this automatically. If we see this log, it's unexpected.
            } else if (part.toolResponse) {
                console.log('Found toolResponse part (should be handled by model):', part.toolResponse);
            }
        }
    }

    if (!textResponse) {
      console.warn('Text response is empty after parsing parts.');
      // Final check if the model intended to call a tool but we didn't get text back
       const toolCall = output?.content?.parts.find(part => part.toolRequest);
       if (toolCall) {
            console.warn("A tool was called, but no text response was generated by the model in the same turn.");
            textResponse = "J'ai utilisé un outil pour chercher l'information, mais je n'ai pas pu formuler de réponse. Pourriez-vous reformuler votre question différemment ?";
       } else {
           console.error('No text response and no tool call detected. Returning generic error.');
           return { content: "Je suis désolé, je n'ai pas pu générer de réponse pour le moment. Veuillez réessayer." };
       }
    }

    console.log('Final textResponse to be returned:', textResponse);
    console.log('======== [ASSISTANT FLOW END] ========');
    return { content: textResponse };
  }
);


export async function chat(input: ChatInput): Promise<ChatOutput> {
    return chatFlow(input);
}
